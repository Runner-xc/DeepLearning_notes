{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focal Loss 损失函数\n",
    "\n",
    "> 论文地址：https://arxiv.org/abs/1708.02002\n",
    "<br>\n",
    "> 代码地址：https://github.com/facebookresearch/Detectron\n",
    "<br>\n",
    "> 参考博文：https://blog.csdn.net/PLANTTHESON/article/details/133933456\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Focal Loss** 是一个在目标检测领域常用的损失函数，它是何凯明大佬在RetinaNet网络中提出的，解决了目标检测中正负样本极不平衡和难分类样本学习的问题。<br> \n",
    "**Focal Loss** 通过引入一个可调节的聚焦参数，将容易分类的样本的权重降低，而将困难样本的权重提高，从而使得模型更加关注困难样本，提高了模型的性能。<br>\n",
    "需要注意的是，Focal Loss损失函数容易受到**噪声**的干扰，因此训练集中标注的信息尽量**不要出现错误**的情况。\n",
    "\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 相关背景理论知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 交叉熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在信息学中 **信息熵(entropy)** 是表示系统的混乱程度和确定性的。**交叉熵（Cross Entropy）** 是一种衡量两个概率分布之间的相似程度的指标。在机器学习中，交叉熵常常用于评估模型预测结果与真实标签之间的差距。交叉熵越小，说明模型预测结果与真实标签越接近，模型性能越好。<br>\n",
    "交叉熵公式如下：\n",
    "$$\n",
    "H(p,q) = -\\sum_{i=1}^{n}p(x_i)\\log q(x_i)\n",
    "$$\n",
    "\n",
    "其中，$p$ 是真实分布，$x_i$ 是预测样本，$q$ 是预测分布，$n$ 是类别数。<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 二分类交叉熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于二分类问题来说，他的交叉熵是：\n",
    "\\begin{aligned}\n",
    "    \\\\\\mathrm{CE_{loss}}(p,y)=\\begin{cases}-\\log(p)&\\text{if } y=1\n",
    "    \\\\-\\log(1-p)&\\text{otherwise}.\\end{cases}\n",
    "\\end{aligned}\n",
    "或者是：\n",
    "$$CE_{loss} = -[y \\cdot log(p) + (1-y) \\cdot log(1-p)]$$\n",
    "其中，p表示y=1的概率，接下来我们来定义 p 的函数：<br>\n",
    "\\begin{aligned}p_{\\mathrm t}=\\begin{cases}p&\\text{if }y=1\\\\1-p&\\text{otherwise},\\end{cases}\\end{aligned}\n",
    "那么，交叉熵可以表示为：<br>\n",
    "\\begin{aligned}\\mathrm{CE_{loss}}(p,y)=\\mathrm{CE_{loss}}(p_\\mathrm{t})=-\\log(p_\\mathrm{t})\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 多分类交叉熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多分类交叉熵公式如下：<br>\n",
    "$$\n",
    "\n",
    "CE_{loss} = - \\sum_{i}^{C} [y_i \\cdot log(p_i) + (1-y_i) \\cdot log(1-p_i)]\n",
    "$$\n",
    "\n",
    "<!-- 如果看作只计算预测正确的概率的交叉熵，那么公式可以简化为：<br>\n",
    "$$\n",
    "CE_{loss} = - \\sum_{i}^{C} log(p_i)\n",
    "$$ -->\n",
    "- $C$ 是类别数\n",
    "- $y_i$ 是第 $i$ 类的标签，如果第 $i$ 类是正样本，那么 $y_i=1$，否则 $y_i=0$\n",
    "- $p_i$ 是第 $i$ 类的预测正确的概率\n",
    "$ 0 < p_i < 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Balanced Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过引入平衡因子α来平衡正负样本的权重。**平衡因子α** （α ∈ [ 0 , 1 ] ）的值通常设置为正样本的数量与负样本数量之间的比值。<br>\n",
    "在计算损失时，对于正样本，其损失权重为1；对于负样本，其损失权重为α。通过这种方式，Balanced Cross Entropy可以平衡正负样本的权重，从而提高模型的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算公式：<br>\n",
    "\\begin{aligned}\\mathrm{CE_{loss}}(p_\\mathrm{t})=-\\alpha_\\mathrm{t}\\log(p_\\mathrm{t})\\end{aligned}\n",
    "对于 α 设定不同值的效果：<br>\n",
    "<center>\n",
    "\n",
    "![img](./imgs/BCE_α.png)\n",
    "</center>\n",
    "论文中当 α=0.75 时，效果最好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Focal Loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focal Loss 是在 Balanced Cross Entropy 的基础上，进一步引入了 **聚焦参数γ**（γ ∈ [ 0 , 5 ]）来调节损失函数的权重。聚焦参数 **γ** 的引入，使得模型更加关注那些被错误分类的样本，从而提高了模型的性能。<br>\n",
    "计算公式如下：<br>\n",
    "\\begin{equation}\n",
    "    \\mathrm{FL_{loss}}(p_\\mathrm{t})=-\\alpha_\\mathrm{t}(1-p_\\mathrm{t})^\\gamma\\log(p_\\mathrm{t}). \n",
    "\\end{equation}\n",
    "\n",
    "由于$$CE_{loss}(p_t) = -log(p_t)$$ 可得：$$p_t = e^{-CE_{loss}} $$所以：$$FL_{loss}(p_t) = {\\alpha}(1-e^{-CE_{loss}})^{\\gamma}(CE_{loss})$$\n",
    "$(1-p_{\\mathrm{t}})^{\\gamma} $ 可以减低易分样本的损失贡献，从而增加难分样本的损失比例，解释如下：当Pt趋向于1，即说明该样本是易区分样本，此时调制因子 $(1-p_{\\mathrm{t}})^{\\gamma} $ 是趋向于0，说明对损失的贡献较小，即减低了易区分样本的损失比例。当 pt 很小，也就是假如某个样本被分到正样本，但是该样本为前景的概率特别小，即被错分到正样本了，此时 调制因子 $(1-p_{\\mathrm{t}})^{\\gamma} $ 是趋向于1，对loss也没有太大的影响。\n",
    "\n",
    "- Focal Loss 的效果：\n",
    "\n",
    "<table style=\"width: 100%; table-layout: fixed;\">\n",
    "    <tr>\n",
    "        <td style=\"text-align: center;\"><img width=\"400px\" src=\"./imgs/FL_γ.png\"></td>\n",
    "        <td style=\"text-align: center;\"><img width=\"400px\" src=\"./imgs/FL_γ_α.png\"></td>\n",
    "        <td style=\"text-align: center;\"><img width=\"400px\" src=\"./imgs/COCO_AP_FL.png\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center;\"> 不同的 γ 值的效果</td>\n",
    "        <td style=\"text-align: center;\"> γ 和 α 对比试验</td>\n",
    "        <td style=\"text-align: center;\"> 不同模型使用FL后，在COCO数据集上的AP表现 </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Focal Loss 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0000e+00, 5.9605e-08, 5.9605e-07,  ..., 1.0000e+00, 1.0000e+00,\n",
       "         1.0000e+00]),\n",
       " tensor([0, 1, 2, 3]),\n",
       " tensor([0, 1]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "output_tensor = torch.rand(1, 4, 128, 128, 128)\n",
    "\n",
    "target_tensor = torch.randint(0, 4, (1, 128, 128, 128))\n",
    "\n",
    "output_onehot = F.one_hot(target_tensor, num_classes=4).permute(0, 4, 1, 2, 3)\n",
    "\n",
    "output_tensor.shape, target_tensor.shape, output_onehot.shape\n",
    "output_tensor.max(), target_tensor.max(), target_tensor.unique(), output_onehot.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected floating point type for target with class probabilities, got Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     20\u001b[0m focal_loss \u001b[38;5;241m=\u001b[39m FocalLoss(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mfocal_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n",
      "File \u001b[0;32m~/miniconda3/envs/cv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[61], line 9\u001b[0m, in \u001b[0;36mFocalLoss.forward\u001b[0;34m(self, pred, targets)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pred, targets):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# 计算交叉熵损失\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     celoss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# 计算调制因子\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     pt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mceloss)\n",
      "File \u001b[0;32m~/miniconda3/envs/cv/lib/python3.10/site-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected floating point type for target with class probabilities, got Long"
     ]
    }
   ],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, pred, targets):\n",
    "        # 计算交叉熵损失\n",
    "        celoss = F.cross_entropy(pred, targets, reduction='mean')\n",
    "\n",
    "        # 计算调制因子\n",
    "        pt = torch.exp(-celoss)\n",
    "        modulating_factor = (1 - pt) ** self.gamma\n",
    "\n",
    "        # 计算加权损失\n",
    "        loss = self.alpha * modulating_factor * celoss\n",
    "\n",
    "        return loss.mean()\n",
    "    \n",
    "focal_loss = FocalLoss(alpha=0.25, gamma=2)\n",
    "loss = focal_loss(output_tensor.float(), target_tensor)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
